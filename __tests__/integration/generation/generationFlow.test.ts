/**
 * Integration Tests: Generation Flow
 *
 * Tests the integration between:
 * - generationService ↔ llmService (token callbacks, generation lifecycle)
 * - generationService ↔ useChatStore (streaming message updates)
 *
 * These tests verify that the services work together correctly,
 * not just that they work in isolation.
 */

import { useChatStore } from '../../../src/stores/chatStore';
import { useAppStore } from '../../../src/stores/appStore';
import { generationService } from '../../../src/services/generationService';
import { llmService } from '../../../src/services/llm';
import { activeModelService } from '../../../src/services/activeModelService';
import {
  resetStores,
  setupWithActiveModel,
  setupWithConversation,
  flushPromises,
  getChatState,
  collectSubscriptionValues,
} from '../../utils/testHelpers';
import { createMessage, createDownloadedModel } from '../../utils/factories';

// Mock the services
jest.mock('../../../src/services/llm');
jest.mock('../../../src/services/activeModelService');

const mockLlmService = llmService as jest.Mocked<typeof llmService>;
const mockActiveModelService = activeModelService as jest.Mocked<typeof activeModelService>;

describe('Generation Flow Integration', () => {
  beforeEach(async () => {
    resetStores();
    jest.clearAllMocks();

    // Setup default mock implementations
    mockLlmService.isModelLoaded.mockReturnValue(true);
    mockLlmService.getLoadedModelPath.mockReturnValue('/mock/path/model.gguf');
    mockLlmService.getGpuInfo.mockReturnValue({
      gpu: false,
      gpuBackend: 'CPU',
      gpuLayers: 0,
      reasonNoGPU: '',
    });
    mockLlmService.getPerformanceStats.mockReturnValue({
      lastTokensPerSecond: 15.5,
      lastDecodeTokensPerSecond: 18.2,
      lastTimeToFirstToken: 0.5,
      lastGenerationTime: 5.0,
      lastTokenCount: 100,
    });
    mockLlmService.stopGeneration.mockResolvedValue();

    mockActiveModelService.getActiveModels.mockReturnValue({
      text: { model: null, isLoaded: true, isLoading: false },
      image: { model: null, isLoaded: false, isLoading: false },
    });

    // Reset generationService state by stopping any in-progress generation
    // This ensures clean state between tests
    await generationService.stopGeneration().catch(() => {});
  });

  describe('generationService → llmService Token Flow', () => {
    it('should stream tokens from llmService to generationService state', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      const tokens = ['Hello', ' ', 'world', '!'];
      let streamCallback: ((token: string) => void) | null = null;
      let completeCallback: (() => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, onStream, onComplete, _onError, _onThinking) => {
          streamCallback = onStream!;
          completeCallback = onComplete!;
          return 'Hello world!';
        }
      );

      // Start generation
      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      const generatePromise = generationService.generateResponse(conversationId, messages);

      // Give time for setup
      await flushPromises();

      // Verify generation started
      expect(generationService.getState().isGenerating).toBe(true);
      expect(generationService.getState().conversationId).toBe(conversationId);

      // Stream tokens
      for (const token of tokens) {
        streamCallback?.(token);
        await flushPromises();
      }

      // Verify streaming content accumulated
      expect(generationService.getState().streamingContent).toBe('Hello world!');

      // Complete generation
      completeCallback?.();
      await generatePromise;

      // Verify state reset
      expect(generationService.getState().isGenerating).toBe(false);
      expect(generationService.getState().streamingContent).toBe('');
    });

    it('should call onFirstToken callback when first token arrives', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      let streamCallback: ((token: string) => void) | null = null;
      let completeCallback: (() => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, onStream, onComplete, _onError, _onThinking) => {
          streamCallback = onStream!;
          completeCallback = onComplete!;
          return 'Test';
        }
      );

      const onFirstToken = jest.fn();
      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      const generatePromise = generationService.generateResponse(conversationId, messages, onFirstToken);

      await flushPromises();

      // First token should trigger callback
      streamCallback?.('First');
      await flushPromises();
      expect(onFirstToken).toHaveBeenCalledTimes(1);

      // Second token should not trigger callback again
      streamCallback?.(' token');
      await flushPromises();
      expect(onFirstToken).toHaveBeenCalledTimes(1);

      completeCallback?.();
      await generatePromise;
    });

    it('should transition isThinking from true to false on first token', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      let streamCallback: ((token: string) => void) | null = null;
      let completeCallback: (() => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, onStream, onComplete, _onError, _onThinking) => {
          streamCallback = onStream!;
          completeCallback = onComplete!;
          return 'Test';
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      const generatePromise = generationService.generateResponse(conversationId, messages);

      await flushPromises();

      // Initially should be thinking
      expect(generationService.getState().isThinking).toBe(true);

      // First token should stop thinking
      streamCallback?.('Hello');
      await flushPromises();
      expect(generationService.getState().isThinking).toBe(false);

      completeCallback?.();
      await generatePromise;
    });
  });

  describe('generationService → chatStore Streaming Updates', () => {
    it('should update chatStore streaming state when generation starts', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      let completeCallback: (() => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, _onStream, onComplete, _onError, _onThinking) => {
          completeCallback = onComplete!;
          return 'Test';
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      const generatePromise = generationService.generateResponse(conversationId, messages);

      await flushPromises();

      // Check chatStore streaming state
      const chatState = getChatState();
      expect(chatState.streamingForConversationId).toBe(conversationId);
      expect(chatState.isThinking).toBe(true);

      completeCallback?.();
      await generatePromise;
    });

    it('should append tokens to chatStore streamingMessage', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      let streamCallback: ((token: string) => void) | null = null;
      let completeCallback: (() => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, onStream, onComplete, _onError, _onThinking) => {
          streamCallback = onStream!;
          completeCallback = onComplete!;
          return 'Hello world';
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      const generatePromise = generationService.generateResponse(conversationId, messages);

      await flushPromises();

      // Stream tokens
      streamCallback?.('Hello');
      await flushPromises();
      expect(getChatState().streamingMessage).toBe('Hello');

      streamCallback?.(' world');
      await flushPromises();
      expect(getChatState().streamingMessage).toBe('Hello world');

      completeCallback?.();
      await generatePromise;
    });

    it('should finalize message in chatStore when generation completes', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      // Setup app store with the model for metadata
      const model = createDownloadedModel({ id: modelId, name: 'Test Model' });
      useAppStore.setState({
        downloadedModels: [model],
        activeModelId: modelId,
      });

      let streamCallback: ((token: string) => void) | null = null;
      let completeCallback: (() => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, onStream, onComplete, _onError, _onThinking) => {
          streamCallback = onStream!;
          completeCallback = onComplete!;
          return 'Complete response';
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      const generatePromise = generationService.generateResponse(conversationId, messages);

      await flushPromises();

      // Stream complete response
      streamCallback?.('Complete response');
      await flushPromises();

      // Complete generation
      completeCallback?.();
      await generatePromise;

      // Verify message was finalized
      const chatState = getChatState();
      expect(chatState.streamingMessage).toBe('');
      expect(chatState.streamingForConversationId).toBe(null);
      expect(chatState.isStreaming).toBe(false);

      // Verify assistant message was added
      const conversation = chatState.conversations.find(c => c.id === conversationId);
      expect(conversation?.messages).toHaveLength(1);
      expect(conversation?.messages[0].role).toBe('assistant');
      expect(conversation?.messages[0].content).toBe('Complete response');
    });

    it('should include generation metadata when finalizing message', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      const model = createDownloadedModel({ id: modelId, name: 'Test Model' });
      useAppStore.setState({
        downloadedModels: [model],
        activeModelId: modelId,
      });

      mockLlmService.getGpuInfo.mockReturnValue({
        gpu: true,
        gpuBackend: 'Metal',
        gpuLayers: 32,
        reasonNoGPU: '',
      });

      mockLlmService.getPerformanceStats.mockReturnValue({
        lastTokensPerSecond: 25.5,
        lastDecodeTokensPerSecond: 30.2,
        lastTimeToFirstToken: 0.3,
        lastGenerationTime: 3.0,
        lastTokenCount: 75,
      });

      let streamCallback: ((token: string) => void) | null = null;
      let completeCallback: (() => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, onStream, onComplete, _onError, _onThinking) => {
          streamCallback = onStream!;
          completeCallback = onComplete!;
          return 'Response';
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      const generatePromise = generationService.generateResponse(conversationId, messages);

      await flushPromises();
      streamCallback?.('Response');
      await flushPromises();
      completeCallback?.();
      await generatePromise;

      const chatState = getChatState();
      const conversation = chatState.conversations.find(c => c.id === conversationId);
      const assistantMessage = conversation?.messages[0];

      expect(assistantMessage?.generationMeta).toBeDefined();
      expect(assistantMessage?.generationMeta?.gpu).toBe(true);
      expect(assistantMessage?.generationMeta?.gpuBackend).toBe('Metal');
      expect(assistantMessage?.generationMeta?.tokensPerSecond).toBe(25.5);
      expect(assistantMessage?.generationMeta?.modelName).toBe('Test Model');
    });

    it('should clear streaming message on error', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      let errorCallback: ((error: Error) => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, _onStream, _onComplete, onError, _onThinking) => {
          errorCallback = onError!;
          throw new Error('Generation failed');
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];

      await expect(
        generationService.generateResponse(conversationId, messages)
      ).rejects.toThrow('Generation failed');

      // Verify streaming state was cleared
      const chatState = getChatState();
      expect(chatState.streamingMessage).toBe('');
      expect(chatState.streamingForConversationId).toBe(null);
      expect(chatState.isStreaming).toBe(false);
    });
  });

  describe('Generation Lifecycle', () => {
    it('should prevent concurrent generations', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      let completeCallback: (() => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, _onStream, onComplete, _onError, _onThinking) => {
          completeCallback = onComplete!;
          // Never complete automatically
          return new Promise(() => {});
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];

      // Start first generation
      generationService.generateResponse(conversationId, messages);
      await flushPromises();

      // Try to start second generation
      await generationService.generateResponse(conversationId, messages);

      // llmService.generateResponse should only be called once
      expect(mockLlmService.generateResponse).toHaveBeenCalledTimes(1);
    });

    it('should throw if no model is loaded', async () => {
      const conversationId = setupWithConversation();

      // Model is not loaded
      mockLlmService.isModelLoaded.mockReturnValue(false);

      const messages = [createMessage({ role: 'user', content: 'Hi' })];

      // The service checks isModelLoaded and throws if false
      let thrownError: Error | null = null;
      try {
        await generationService.generateResponse(conversationId, messages);
      } catch (error) {
        thrownError = error as Error;
      }

      expect(thrownError).not.toBeNull();
      expect(thrownError?.message).toBe('No model loaded');
    });

    it('should handle stopGeneration correctly', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      let streamCallback: ((token: string) => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, onStream, _onComplete, _onError, _onThinking) => {
          streamCallback = onStream!;
          // Simulate long running generation by returning a never-resolving promise
          await new Promise(() => {});
          return 'never reached';
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];

      // Start generation (don't await - it never completes)
      generationService.generateResponse(conversationId, messages);

      // Wait for generation to start
      await flushPromises();

      // Verify generation started
      expect(generationService.getState().isGenerating).toBe(true);

      // Stream some content - this updates the service's internal streamingContent
      streamCallback?.('Partial');
      await flushPromises();
      streamCallback?.(' response');
      await flushPromises();

      // Verify content was streamed
      expect(generationService.getState().streamingContent).toBe('Partial response');

      // Stop generation - should return the accumulated content
      const partialContent = await generationService.stopGeneration();

      expect(partialContent).toBe('Partial response');
      expect(mockLlmService.stopGeneration).toHaveBeenCalled();
      expect(generationService.getState().isGenerating).toBe(false);
    });

    it('should save partial response when stopped with content', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      const model = createDownloadedModel({ id: modelId, name: 'Test Model' });
      useAppStore.setState({
        downloadedModels: [model],
        activeModelId: modelId,
      });

      let streamCallback: ((token: string) => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, onStream, _onComplete, _onError, _onThinking) => {
          streamCallback = onStream!;
          return new Promise(() => {});
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      generationService.generateResponse(conversationId, messages);

      await flushPromises();

      // Stream some content
      streamCallback?.('Partial response here');
      await flushPromises();

      // Stop generation
      await generationService.stopGeneration();

      // Verify partial response was saved
      const chatState = getChatState();
      const conversation = chatState.conversations.find(c => c.id === conversationId);
      expect(conversation?.messages).toHaveLength(1);
      expect(conversation?.messages[0].content).toBe('Partial response here');
    });

    it('should not save message when stopped with empty content', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, _onStream, _onComplete, _onError, _onThinking) => {
          return new Promise(() => {});
        }
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      generationService.generateResponse(conversationId, messages);

      await flushPromises();

      // Stop without any tokens streamed
      await generationService.stopGeneration();

      // Verify no message was saved
      const chatState = getChatState();
      const conversation = chatState.conversations.find(c => c.id === conversationId);
      expect(conversation?.messages).toHaveLength(0);
    });
  });

  describe('State Subscription', () => {
    it('should notify subscribers of state changes', async () => {
      const modelId = setupWithActiveModel();
      const conversationId = setupWithConversation({ modelId });

      let streamCallback: ((token: string) => void) | null = null;
      let completeCallback: (() => void) | null = null;

      mockLlmService.generateResponse.mockImplementation(
        async (_messages, onStream, onComplete, _onError, _onThinking) => {
          streamCallback = onStream!;
          completeCallback = onComplete!;
          return 'Test';
        }
      );

      const { values, unsubscribe } = collectSubscriptionValues(
        (listener) => generationService.subscribe(listener)
      );

      const messages = [createMessage({ role: 'user', content: 'Hi' })];
      const generatePromise = generationService.generateResponse(conversationId, messages);

      await flushPromises();
      streamCallback?.('Token');
      await flushPromises();
      completeCallback?.();
      await generatePromise;

      unsubscribe();

      // Should have received multiple state updates
      expect(values.length).toBeGreaterThan(1);

      // First update after initial state should show generating
      const generatingState = values.find(v => v.isGenerating);
      expect(generatingState).toBeDefined();

      // Should have a state with content
      const contentState = values.find(v => v.streamingContent === 'Token');
      expect(contentState).toBeDefined();

      // Last state should be idle
      const lastState = values[values.length - 1];
      expect(lastState.isGenerating).toBe(false);
    });
  });
});
